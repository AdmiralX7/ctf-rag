{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e87c09c7",
   "metadata": {},
   "source": [
    "# Qualitative Evaluation: RAG vs. Non-RAG Assistant\n",
    "\n",
    "**Objective:** To perform a qualitative comparison between answers generated by a standard Large Language Model (Gemini 2.5 Pro) and the RAG-augmented assistant (`ask.py`).\n",
    "\n",
    "**Methodology:**\n",
    "This notebook will execute a series of predefined cybersecurity and CTF-related questions against two distinct pipelines:\n",
    "1.  **Non-RAG:** The question is sent directly to the Gemini 2.5 Pro model without any external context.\n",
    "2.  **RAG:** The question is processed by the `ask.py` pipeline, which first retrieves relevant context from the custom MongoDB/Vertex AI database and then injects that context into the prompt sent to the Gemini model.\n",
    "\n",
    "The goal is not to assign a quantitative score, but to qualitatively observe the differences in the generated answers and to verify that the RAG pipeline is successfully retrieving relevant source documents. The \"Sources Retrieved by RAG\" for each query provides evidence that the retrieval mechanism is functioning as designed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b46b8d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded.\n",
      "Clients initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "from dotenv import load_dotenv\n",
    "import vertexai\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from google.cloud import aiplatform\n",
    "from pymongo import MongoClient\n",
    "import vertexai.generative_models as generative_models\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Configuration ---\n",
    "# Ensure the .env file is in the root of the project directory\n",
    "load_dotenv()\n",
    "PROJECT_ID = os.getenv(\"GCP_PROJECT_ID\")\n",
    "LOCATION = \"us-central1\"\n",
    "EMBEDDING_MODEL_NAME = \"text-embedding-005\"\n",
    "DETAILED_ENDPOINT_NAME = os.getenv(\"DETAILED_ENDPOINT_NAME\", \"ctf-detailed-endpoint\")\n",
    "MONGO_URI = os.getenv(\"MONGO_URI\", \"mongodb://localhost:27017/\")\n",
    "DB_NAME = \"ctf_writeups_db\"\n",
    "COLLECTION_NAME = \"writeups\"\n",
    "GEMINI_MODEL_NAME = \"gemini-2.5-pro\" # Using the Pro model as decided\n",
    "\n",
    "print(\"Configuration loaded.\")\n",
    "\n",
    "# --- Client Initialization ---\n",
    "# Initialize clients once to avoid re-creation in loops\n",
    "try:\n",
    "    vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "    mongo_client = MongoClient(MONGO_URI)\n",
    "    embedding_model = TextEmbeddingModel.from_pretrained(EMBEDDING_MODEL_NAME)\n",
    "    generative_model = generative_models.GenerativeModel(GEMINI_MODEL_NAME)\n",
    "    print(\"Clients initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during client initialization: {e}\")\n",
    "\n",
    "\n",
    "# --- Function Definitions (Adapted from ask.py) ---\n",
    "\n",
    "def get_embedding(text: str) -> list[float]:\n",
    "    \"\"\"Generates a numerical vector embedding for a given text string.\"\"\"\n",
    "    embeddings = embedding_model.get_embeddings([text])\n",
    "    return embeddings[0].values\n",
    "\n",
    "def get_vector_search_neighbors(embedding: list[float], num_neighbors: int = 1) -> list[str]:\n",
    "    \"\"\"\n",
    "    Performs a vector search to find the most similar document chunks.\n",
    "    Note: We are now only retrieving the TOP 1 neighbor to test the single-context hypothesis.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        endpoints = aiplatform.MatchingEngineIndexEndpoint.list(\n",
    "            filter=f'display_name=\"{DETAILED_ENDPOINT_NAME}\"',\n",
    "            project=PROJECT_ID,\n",
    "            location=LOCATION\n",
    "        )\n",
    "        if not endpoints:\n",
    "            raise RuntimeError(f\"Endpoint with display name '{DETAILED_ENDPOINT_NAME}' not found.\")\n",
    "        \n",
    "        endpoint_resource_name = endpoints[0].resource_name\n",
    "        endpoint = aiplatform.MatchingEngineIndexEndpoint(index_endpoint_name=endpoint_resource_name)\n",
    "        deployed_index_id = endpoint.deployed_indexes[0].id\n",
    "        \n",
    "        response = endpoint.find_neighbors(\n",
    "            deployed_index_id=deployed_index_id,\n",
    "            queries=[embedding],\n",
    "            num_neighbors=num_neighbors\n",
    "        )\n",
    "        \n",
    "        if not response or not response[0]:\n",
    "            return []\n",
    "        \n",
    "        return [neighbor.id for neighbor in response[0]]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in vector search: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def parse_and_deduplicate_ids(neighbor_ids: list[str]) -> list[str]:\n",
    "    \"\"\"Parses ctftime_id from chunk IDs and removes duplicates.\"\"\"\n",
    "    return list({neighbor_id.split('_')[0] for neighbor_id in neighbor_ids})\n",
    "\n",
    "def fetch_documents_from_mongodb(doc_ids: list[str]) -> tuple[str, list[dict]]:\n",
    "    \"\"\"\n",
    "    Fetches documents from MongoDB. Returns a clean context string containing only the\n",
    "    AI-generated summaries and a list of source dictionaries for citation.\n",
    "    \"\"\"\n",
    "    if not doc_ids:\n",
    "        return \"\", []\n",
    "    try:\n",
    "        db = mongo_client[DB_NAME]\n",
    "        collection = db[COLLECTION_NAME]\n",
    "        documents = list(collection.find({\"ctftime_id\": {\"$in\": doc_ids}}))\n",
    "        \n",
    "        context_parts = []\n",
    "        sources = []\n",
    "\n",
    "        for doc in documents:\n",
    "            # Build context for the LLM with the full text of the top write-up\n",
    "            if doc.get('rewritten_full_text'):\n",
    "                context_parts.append(doc['rewritten_full_text'])\n",
    "            \n",
    "            # Collect source information for citation\n",
    "            sources.append({\n",
    "                \"id\": doc.get('ctftime_id'),\n",
    "                \"title\": doc.get('title', 'N/A'),\n",
    "                \"url\": doc.get('url', 'N/A')\n",
    "            })\n",
    "            \n",
    "        # Join summaries with a separator to distinguish them\n",
    "        context = \"\\\\n\\\\n---\\\\n\\\\n\".join(context_parts)\n",
    "        return context, sources\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching from MongoDB: {e}\")\n",
    "        return \"\", []\n",
    "\n",
    "def get_rag_answer(context: str, question: str) -> str:\n",
    "    \"\"\"Constructs a RAG prompt and calls the Gemini model.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Based on the following context from cybersecurity write-ups, provide a concise answer to the user's question.\n",
    "\n",
    "Context:\n",
    "---\n",
    "{context}\n",
    "---\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = generative_model.generate_content(prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        return f\"Error generating RAG answer: {e}\"\n",
    "\n",
    "def get_no_rag_answer(question: str) -> str:\n",
    "    \"\"\"Calls the Gemini model directly without any context.\"\"\"\n",
    "    try:\n",
    "        response = generative_model.generate_content(question)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        return f\"Error generating non-RAG answer: {e}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad30e65",
   "metadata": {},
   "source": [
    "### Evaluation Queries\n",
    "\n",
    "Here we define a list of questions to test the two systems. These questions are designed to be highly specific to the content within the RAG database, in order to clearly demonstrate the advantage of the context-augmented approach over a generic LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d5ac6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_queries = [\n",
    "    # Queries from 40147 (debug-2)\n",
    "    \"In the 'debug-2' pwn challenge, what is the purpose of the `case_swap` function and how does it corrupt the initial ROP payload before the stack pivot to the `.bss` section?\",\n",
    "    \"For the 'debug-2' exploit, after leaking the libc base address using `puts`, what specific `one_gadget` offset was used to achieve a shell, and what were its constraints?\",\n",
    "    \n",
    "    # Queries from 40255 (Broken Trust)\n",
    "    \"Explain the use-after-free vulnerability in the OP-TEE kernel's `crypto_hash_ctx` object in the 'Broken Trust' challenge. How was the `tee_ta_close_session` call used to trigger the UAF?\",\n",
    "    \"In the 'Broken Trust' exploit, which virtual function table pointer was hijacked, and what specific `SMC` (Secure Monitor Call) handler was invoked to gain privileged execution?\",\n",
    "\n",
    "    # Queries from 40075 (G0tchaberg)\n",
    "    \"Describe the race condition in the Gotenberg PDF conversion service in the 'G0tchaberg' challenge. How were concurrent requests with different `waitDelay` parameters used to leak the temporary file path of the PDF?\",\n",
    "    \n",
    "    # Queries from 40309 (dont_whisper)\n",
    "    \"In the 'dont_whisper' challenge, what specific adversarial technique, like using non-ASCII characters or homoglyphs, was used to bypass the initial filter and achieve command injection in the Whisper model's transcription output?\",\n",
    "\n",
    "    # Queries from 40024 (McFlagChecker)\n",
    "    \"In the 'McFlagChecker' Minecraft datapack challenge, what were the four distinct mathematical transformations applied to the player's input score, and in what order were they reversed to find the flag?\",\n",
    "    \"What was the purpose of the Linear Congruential Generator (LCG) with its specific multiplier and increment values in the 'McFlagChecker' challenge, and how was its state reversed?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acef66d6",
   "metadata": {},
   "source": [
    "### Execute Comparison\n",
    "\n",
    "The following loop will now process each question. For each one, it will:\n",
    "1.  Run the RAG pipeline to generate an answer and identify the source documents.\n",
    "2.  Run the Non-RAG query to generate a baseline answer.\n",
    "3.  Save the comparison to a unique Markdown file in a timestamped output directory for later analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04c71842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving evaluation output to: evaluation_run_2025-07-14_15-05-28\n",
      "  - Saved: evaluation_run_2025-07-14_15-05-28/01_in_the_debug-2_pwn_challenge_what_is_the_purpose_o.md\n",
      "  - Saved: evaluation_run_2025-07-14_15-05-28/02_for_the_debug-2_exploit_after_leaking_the_libc_bas.md\n",
      "  - Saved: evaluation_run_2025-07-14_15-05-28/03_explain_the_use-after-free_vulnerability_in_the_op.md\n",
      "  - Saved: evaluation_run_2025-07-14_15-05-28/04_in_the_broken_trust_exploit_which_virtual_function.md\n",
      "  - Saved: evaluation_run_2025-07-14_15-05-28/05_describe_the_race_condition_in_the_gotenberg_pdf_c.md\n",
      "  - Saved: evaluation_run_2025-07-14_15-05-28/06_in_the_dont_whisper_challenge_what_specific_advers.md\n",
      "  - Saved: evaluation_run_2025-07-14_15-05-28/07_in_the_mcflagchecker_minecraft_datapack_challenge_.md\n",
      "  - Saved: evaluation_run_2025-07-14_15-05-28/08_what_was_the_purpose_of_the_linear_congruential_ge.md\n",
      "\n",
      "Evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# Create a timestamped directory for this evaluation run\n",
    "run_timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "output_dir = f\"evaluation_run_{run_timestamp}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Saving evaluation output to: {output_dir}\")\n",
    "\n",
    "for i, query in enumerate(evaluation_queries):\n",
    "    # --- RAG Pipeline ---\n",
    "    question_embedding = get_embedding(query)\n",
    "    neighbor_ids = get_vector_search_neighbors(question_embedding)\n",
    "    unique_doc_ids = parse_and_deduplicate_ids(neighbor_ids)\n",
    "\n",
    "    # --- Non-RAG Pipeline ---\n",
    "    no_rag_answer = get_no_rag_answer(query)\n",
    "\n",
    "    # --- Generate Formatted Output ---\n",
    "    output_md = f\"\"\"\n",
    "### Question: {query}\n",
    "---\n",
    "<br>\n",
    "\n",
    "**RAG-Augmented Answer:**\n",
    "\"\"\"\n",
    "    if unique_doc_ids:\n",
    "        context, sources = fetch_documents_from_mongodb(unique_doc_ids)\n",
    "        rag_answer = get_rag_answer(context, query)\n",
    "        output_md += f\"\\n{rag_answer}\\n\\n\"\n",
    "        output_md += \"**Sources Retrieved by RAG:**\\n\"\n",
    "        for source in sources:\n",
    "            output_md += f\"*   Document ID: `{source['id']}`\\n\"\n",
    "\n",
    "        if sources:\n",
    "            output_md += \"\\\\n**For more details, you can read the full write-ups:**\\\\n\"\n",
    "            for source in sources:\n",
    "                doc_id = source.get('id')\n",
    "                title = source.get('title')\n",
    "                url = source.get('url')\n",
    "\n",
    "                # If the title is missing, create a sensible default.\n",
    "                if not title or title == 'N/A':\n",
    "                    title = f\"Write-up for CTFtime ID {doc_id}\"\n",
    "                \n",
    "                # If the URL is missing, construct it from the document ID.\n",
    "                if not url or url == 'N/A':\n",
    "                    url = f\"http://ctftime.org/writeup/{doc_id}\"\n",
    "\n",
    "                output_md += f\"*   [{title}]({url})\\\\n\"\n",
    "    else:\n",
    "        output_md += \"\\\\n*No relevant context was found in the database for this query.*\\\\n\"\n",
    "\n",
    "    output_md += f\"\"\"\n",
    "<br>\n",
    "\n",
    "---\n",
    "**Standard LLM Answer (No RAG):**\n",
    "\n",
    "{no_rag_answer}\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "    \n",
    "    # --- Save to File ---\n",
    "    # Create a filesystem-safe slug from the query\n",
    "    slug = re.sub(r'[^\\w-]', '', query.lower().replace(' ', '_'))[:50]\n",
    "    filename = f\"{i+1:02d}_{slug}.md\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "    try:\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(output_md)\n",
    "        print(f\"  - Saved: {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  - Failed to save {filepath}: {e}\")\n",
    "\n",
    "print(\"\\nEvaluation complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
